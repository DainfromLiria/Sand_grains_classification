{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04\n",
    "## Balancing and binning\n",
    "\n",
    "The theoretical background is explained in Lecture 7. CNN and ENN can be found in Lecture 6.\n",
    "\n",
    "### Balancing\n",
    "\n",
    "Problem: We have an instance of some classification problem and our data set is imbalanced, that is, one class has _significantly_ less representants than another class.\n",
    "\n",
    "A model trained on such a dataset can be heavily biased towards the overrepresented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm, ttest_ind\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, RFECV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer, mean_squared_error, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification of default payment (-> credible or not credible clients)\n",
    "data = pd.read_csv('data.csv',sep=';') \n",
    "# source: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "# X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "\n",
    "# based on data description missing values can be indentified as '0'\n",
    "data[['X3', 'X4']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dafault payment? -> 1 = Yes, 0 = No\n",
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Binary) classification and its evaluation\n",
    "\n",
    "In order to evaluate the preprocessing of our data set, we shall use some classifiers as black boxes.\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# code taken from http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"k-Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"MLP\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1), \n",
    "    DecisionTreeClassifier(max_depth=5), # gini criterion\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=RANDOM_STATE)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following metrics are commonly used to evaluate the performance of a **binary** classifier (and the underlying data set).\n",
    "\n",
    "There are 2 classes, positive and negative. The counts of their representants are denoted $P$ and $N$, respectively.\n",
    "The results of the classification are given by the following counts:\n",
    "$$\n",
    "TP = \\text{true positive}, TN = \\text{true negative}, FP = \\text{false positive}, TN = \\text{false negative}.\n",
    "$$\n",
    "(That is, $TP+FP$ samples are assigned the positive class by the selected classifier.)\n",
    "\n",
    "These number are ofter stored in _confusion matrix_ (in Czech often \"klasifikační matice\" or \"matice záměn\"):\n",
    "$$\n",
    "C = \\begin{pmatrix} TP & FN \\\\ FP & TN \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "<img src=\"pics/confusion_matrix.png\">\n",
    "\n",
    "Source: www.info.univ-angers.fr\n",
    "\n",
    "(In multiple class problem, these metrics are generalized in a straightforward manner, see for instance http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "Note that sklearn.metrics.confusion_matrix shows matrix in this manner:\n",
    "$$\n",
    "C = \\begin{pmatrix} TN & FP \\\\ FN & TP \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the confusion matrix by http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "y = list(data['class'])\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=RANDOM_STATE)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a classifier above and classify `data` \n",
    "# and calculate the confusion matrix of the result\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf = SVC(gamma=2, C=1) # depends on C, can be tuned by https://scikit-learn.org/stable/modules/grid_search.html\n",
    "#clf = SVC(kernel=\"linear\", C=0.025)\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred) # check the order -> (y_true, y_pred) \n",
    "\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix (to have some fancy visualisation ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = (cm.max() + cm.min()) / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = [0,1] # for printing purposes only\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix one can compute:\n",
    "\n",
    "- **True positive rate (TPR)** also known as sensitivity or recall or hit rate,\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP+FN}.\n",
    "$$\n",
    "- **False positive rate (FPR)** also known as false alarm rate or type I error rate,\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP+TN}.\n",
    "$$\n",
    "- **False negative rate (FNR)** also known as miss rate or type II error rate,\n",
    "$$\n",
    "\\text{FNR} = \\frac{FN}{TP+FN}.\n",
    "$$\n",
    "- **True negative rate (TNR)** also known as specificity or selectivity,\n",
    "$$\n",
    "\\text{TNR} = \\frac{TN}{FP+TN}.\n",
    "$$\n",
    "\n",
    "\n",
    "Other important performance measures derivable from the confusion matrix and very often used is the **accuracy**:\n",
    "$$\n",
    " \\text{ACC} = \\frac{TP+TN}{P+N},\n",
    "$$\n",
    "and **F1-score** (harmonic mean of precision and recall):\n",
    "$$\n",
    "\\text{F1-score} = \\frac{2}{1/PPV + 1/TPR},\n",
    "$$\n",
    "where PPV stands for **precision**:\n",
    "$$\n",
    "\\text{PPV} = \\frac{TP}{TP+FP}.\n",
    "$$\n",
    "\n",
    "Especially F1-score is useful in highly unbalanced datasets.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced classes\n",
    "\n",
    "(We still focus on the binary class problem.)\n",
    "\n",
    "The class with the majority of samples is called the **majority class**, the other is the **minority class**.\n",
    "\n",
    "The basic strategies are\n",
    "1. to under-sample the majority class (= to remove some its elements)\n",
    "1. to over-sample the minority class (= to add some elements to the class)\n",
    "1. do both of the above\n",
    "1. do nothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced-learn\n",
    "\n",
    "https://imbalanced-learn.org/stable/\n",
    "\n",
    "`pip/pip3 install imbalanced-learn --user [--upgrade]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train) # or fit_sample\n",
    "\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_res))\n",
    "\n",
    "y_pred = clf.fit(X_res, y_res).predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-sampling methods\n",
    "\n",
    "- Random under-sampling\n",
    "- (Wilson's) Edited Nearest Neighbor (ENN)\n",
    "- Condensed Nearest Neighbor (CNN)\n",
    "- One-side Sampling (OSS)\n",
    "- Neighborhood Cleaning Rule (NCL) \n",
    "- Tomek Links\n",
    "\n",
    "https://imbalanced-learn.org/stable/under_sampling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tomek links\n",
    "\n",
    "Let $x$ and $y$ be two samples of distinct class.\n",
    "If there is no sample $z$ such that\n",
    "$$\n",
    "\\rm{d}(x, z) < \\rm{d}(x, y) \\quad \\text{ or } \\quad \\rm{d}(y, z) < \\rm{d}(x, y),\n",
    "$$\n",
    "where $\\rm d$ is **the distance** between the two samples, we say there is a **Tomek link** between $x$ and $y$.\n",
    "\n",
    "In other words, a Tomek’s link exists if the two samples of distinct class are the nearest neighbors of each other.\n",
    "See the illustration at https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html#tomek-links\n",
    "\n",
    "We can choose various strategies how to resample the data using Tomek links, for instance, we might remove all majority samples which are paired by the link with a minority sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with 2D data\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from numpy import where\n",
    "\n",
    "# generate blob data\n",
    "X_exm, y_exm = make_blobs(n_samples=[10000, 500], n_features=2, center_box=(0, 8), random_state=RANDOM_STATE)\n",
    "\n",
    "X_train_exm, X_test_exm, y_train_exm, y_test_exm = train_test_split(X_exm, y_exm, test_size=.4, random_state=RANDOM_STATE)\n",
    "\n",
    "print(Counter(y_train_exm))\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf = SVC(gamma=2, C=1)\n",
    "#clf = SVC(kernel=\"linear\", C=0.025)\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "print('No sampling method applied')\n",
    "\n",
    "idx_class_0 = y_exm == 0\n",
    "_size = 2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_exm[idx_class_0, 0], X_exm[idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 0')\n",
    "plt.scatter(X_exm[~idx_class_0, 0], X_exm[~idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 1')\n",
    "\n",
    "plt.title('Raw unsampled data')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# no sampling method applied\n",
    "y_pred = clf.fit(X_train_exm, y_train_exm).predict(X_test_exm)\n",
    "cnf_matrix = confusion_matrix(y_test_exm, y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix   \n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "print('Accuracy: ', accuracy_score(y_test_exm, y_pred),'\\nF1-score: ', f1_score(y_test_exm, y_pred))\n",
    "\n",
    "\n",
    "###################################\n",
    "# Apply the random under-sampler\n",
    "\n",
    "print('\\n**************************************************')\n",
    "print('Under-sampling using random under-sampler')\n",
    "\n",
    "rus = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train_exm, y_train_exm)\n",
    "idx_resampled = rus.sample_indices_\n",
    "\n",
    "print(Counter(y_resampled))\n",
    "\n",
    "idx_samples_removed = np.setdiff1d(np.arange(X_resampled.shape[0]),\n",
    "                                   idx_resampled)\n",
    "\n",
    "idx_class_0 = y_resampled == 0\n",
    "_size = 3\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_resampled[idx_class_0, 0], X_resampled[idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 0')\n",
    "plt.scatter(X_resampled[~idx_class_0, 0], X_resampled[~idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 1')\n",
    "plt.scatter(X_exm[idx_samples_removed, 0], X_exm[idx_samples_removed, 1],\n",
    "            alpha=.8, s=_size, label='Removed samples', color='red')\n",
    "\n",
    "plt.title('Under-sampling using random under-sampler')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# classify\n",
    "y_pred = clf.fit(X_resampled, y_resampled).predict(X_test_exm)\n",
    "cnf_matrix = confusion_matrix(y_test_exm, y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix   \n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "print('Accuracy: ', accuracy_score(y_test_exm, y_pred),'\\nF1-score: ', f1_score(y_test_exm, y_pred))\n",
    "\n",
    "\n",
    "###################################\n",
    "# Apply the Tomek Link under-sampler\n",
    "\n",
    "print('\\n**************************************************')\n",
    "print('Under-sampling using Tomek Links')\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='not minority') # default - will remove the sample from the majority class\n",
    "X_resampled, y_resampled = tl.fit_resample(X_train_exm, y_train_exm)\n",
    "idx_resampled = tl.sample_indices_\n",
    "\n",
    "print(Counter(y_resampled))\n",
    "\n",
    "idx_samples_removed = np.setdiff1d(np.arange(X_resampled.shape[0]),\n",
    "                                   idx_resampled)\n",
    "\n",
    "idx_class_0 = y_resampled == 0\n",
    "_size = 3\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_resampled[idx_class_0, 0], X_resampled[idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 0')\n",
    "plt.scatter(X_resampled[~idx_class_0, 0], X_resampled[~idx_class_0, 1],\n",
    "            alpha=.8, s=_size, label='Class 1')\n",
    "plt.scatter(X_exm[idx_samples_removed, 0], X_exm[idx_samples_removed, 1],\n",
    "            alpha=.8, s=_size, label='Removed samples', color='red')\n",
    "\n",
    "plt.title('Under-sampling using Tomek Links')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# clsssify\n",
    "y_pred = clf.fit(X_resampled, y_resampled).predict(X_test_exm)\n",
    "cnf_matrix = confusion_matrix(y_test_exm, y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix   \n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "print('Accuracy: ', accuracy_score(y_test_exm, y_pred),'\\nF1-score: ', f1_score(y_test_exm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling methods\n",
    "\n",
    "- random over-sampling\n",
    "- Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "- data augmentation by GANs etc.\n",
    "\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n",
    "\n",
    "#### SMOTE\n",
    "\n",
    "SMOTE inserts new samples based on the following idea: having selected a sample $x$ from the minority class, find its $k$ nearest neighbors, connect them and $x$ by a line, and then pick new samples randomly on each line.\n",
    "\n",
    "<img width=\"300px\" src=\"pics/smote.png\">\n",
    "\n",
    "##### Mathematical formulation\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#mathematical-formulation\n",
    "\n",
    "<!--\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/over-sampling/plot_smote.html#sphx-glr-auto-examples-over-sampling-plot-smote-py\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various under and over-sampling methods on data\n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf = DecisionTreeClassifier()\n",
    "#clf = BernoulliNB()\n",
    "clf = KNeighborsClassifier(3)\n",
    "\n",
    "resamplers = [\n",
    "    (RandomUnderSampler(random_state=RANDOM_STATE),'Random under-sampling'),\n",
    "    (TomekLinks(),'Tomek Links'),\n",
    "    #(CondensedNearestNeighbour(random_state=RANDOM_STATE),'Condensed NN'), # this takes ages\n",
    "    (EditedNearestNeighbours(),'Edited NN'),\n",
    "    (OneSidedSelection(random_state=RANDOM_STATE),'One Sided Selection'),\n",
    "    (RandomOverSampler(random_state=RANDOM_STATE),'Random over-sampling'),\n",
    "    (SMOTE(random_state=RANDOM_STATE),'SMOTE'),\n",
    "    (SMOTEENN(random_state=RANDOM_STATE),'SMOTEENN'),  \n",
    "]\n",
    "\n",
    "# no sampling method applied\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('No sampling method applied')\n",
    "print(Counter(y_train))\n",
    "# Plot normalized confusion matrix   \n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# let's introduce various sampling methods\n",
    "for resampler,description in resamplers:\n",
    "    print('\\n************************************************************')\n",
    "    print(description)\n",
    "    %time X_res, y_res = resampler.fit_resample(X_train, y_train)\n",
    "    print(Counter(y_train))\n",
    "    print(Counter(y_res))\n",
    "\n",
    "    y_pred = clf.fit(X_res, y_res).predict(X_test)   \n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "#     # Plot non-normalized confusion matrix\n",
    "#     plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "#                           title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix   \n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "Binning is a very common discretization technique:\n",
    "1. select bins $B_i$ covering (disjointly) the range of a feature $x$\n",
    "1. create the discretized feature with the value $v_i$ if $x \\in B_i$\n",
    "\n",
    "Usual choices of the bins:\n",
    "1. equal width (the bins have the same size) - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "1. equal depth/frequency (the bins contain the same number of elements) - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "\n",
    "Also you can find some in scikit-learn as [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer).\n",
    "\n",
    "Equal width and equal depth binnings are unsupervised methods, i.e. the choice of bins does not depend on target feature. Compared to that supervised binning methods take into account the target feature when selecting discretization cut points. E.g. entropy-based binning or advanced minimum description length principle ([MDLP](https://github.com/maxpumperla/entropy-mdlp)) algorithm based on [Fayyad, U., & Irani, K. (1993). Multi-interval discretization of continuous-valued attributes for classification learning](https://www.ijcai.org/Proceedings/93-2/Papers/022.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's discretize feature X5 (Age (year))\n",
    "data = data.rename({'X5': 'Age'}, axis=1) \n",
    "display(data[['Age']].describe())\n",
    "data.Age.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do it in a hand\n",
    "\n",
    "data['AgeBin'] = 0 # create a column of 0\n",
    "data.loc[((data['Age'] > 20) & (data['Age'] < 30)) , 'AgeBin'] = 1\n",
    "data.loc[((data['Age'] >= 30) & (data['Age'] < 40)) , 'AgeBin'] = 2\n",
    "data.loc[((data['Age'] >= 40) & (data['Age'] < 50)) , 'AgeBin'] = 3\n",
    "data.loc[((data['Age'] >= 50) & (data['Age'] < 60)) , 'AgeBin'] = 4\n",
    "data.loc[((data['Age'] >= 60) & (data['Age'] < 70)) , 'AgeBin'] = 5\n",
    "data.loc[((data['Age'] >= 70) & (data['Age'] < 81)) , 'AgeBin'] = 6\n",
    "data.AgeBin.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of bins with equal width\n",
    "\n",
    "bins = [20, 29, 39, 49, 59, 69, 81]\n",
    "bins_names = [1, 2, 3, 4, 5, 6]\n",
    "data['AgeBin2'] = pd.cut(data['Age'], bins, labels=bins_names)\n",
    "data.AgeBin2.hist()\n",
    "data.AgeBin2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AgeBin3'] = pd.cut(data['Age'], 6) # bins = 6\n",
    "data.AgeBin3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AgeBin3'] = pd.cut(data['Age'], 6, labels=bins_names)\n",
    "data.AgeBin3.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of bins with equal depth\n",
    "\n",
    "data['AgeBin4'], bins = pd.qcut(data['Age'], 6, labels=bins_names, retbins=True)\n",
    "data.AgeBin4.hist()\n",
    "display(data.AgeBin4.value_counts())\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare discretization (AgeBin3, AgeBin4) by classification model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Age - original data\n",
    "y = list(data['class'])\n",
    "X = data.drop(['class', 'AgeBin', 'AgeBin2', 'AgeBin3', 'AgeBin4'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=RANDOM_STATE)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print('\\nOriginal data \\nAccuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# AgeBin3\n",
    "X = data.drop(['class', 'Age', 'AgeBin', 'AgeBin2', 'AgeBin4'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=RANDOM_STATE)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print('\\nBins with equal width \\nAccuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# AgeBin4\n",
    "X = data.drop(['class', 'Age', 'AgeBin', 'AgeBin2', 'AgeBin3'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=RANDOM_STATE)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print('\\nBins with equal depth \\nAccuracy: ', accuracy_score(y_test, y_pred),'\\nF1-score: ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, there is a cute mistake in this process (methodical one). Can you find it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some References\n",
    "\n",
    "1. https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "1. https://svds.com/learning-imbalanced-classes/\n",
    "1. [Tomas Borovicka, Marcel Jirina Jr., Pavel Kordik and Marcel Jirina, _Selecting Representative Data Sets_](https://www.intechopen.com/books/advances-in-data-mining-knowledge-discovery-and-applications/selecting-representative-data-sets)\n",
    "1. [Vuk, Curk: _ROC Curve, Lift Chart and Calibration Plot_](http://www.stat.wvu.edu/~jharner/courses/dsci503/docs/vuk.pdf)\n",
    "1. [Elhassan T, Aljurf M, Al-Mohanna F, and Shoukri M:_Classification of Imbalance Data using Tomek Link (T-Link) Combined with Random Under-sampling (RUS) as a Data Reduction Method_](https://pdfs.semanticscholar.org/6ec4/18f9071f3a96d5548e87e34be3665703119e.pdf?_ga=2.37521248.1702047165.1541793547-736740435.1531120151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
